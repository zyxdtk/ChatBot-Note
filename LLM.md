# LLM


## 大模型

- [Deep Reinforcement Learning from Human Preferences](https://proceedings.neurips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf)
- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/pdf/2212.08073.pdf)
- [Scaling Laws for Reward Model Overoptimization](https://arxiv.org/pdf/2210.10760.pdf)
- [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/pdf/2303.17564.pdf)
- [MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models](https://github.com/Vision-CAIR/MiniGPT-4)   [demo](https://minigpt-4.github.io/)

- [TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION](https://arxiv.org/pdf/2108.12409.pdf)
- [让研究人员绞尽脑汁的Transformer位置编码](https://kexue.fm/archives/8130)

## RLHF

- [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862)

## Embedding

- [SGPT: GPT Sentence Embeddings for Semantic Search](https://arxiv.org/pdf/2202.08904v5.pdf)
- [Text and Code Embeddings by Contrastive Pre-Training](https://arxiv.org/pdf/2201.10005.pdf)   contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code
- [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/pdf/2004.04906.pdf)
- [sbert](www.sbert.net) 句向量模型
- [Title:Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)  [sentence-transformers](https://github.com/UKPLab/sentence-transformers)
- [Contrastive-Learning-NLP-Papers](https://github.com/ryanzhumich/Contrastive-Learning-NLP-Papers) Paper List for Contrastive Learning for Natural Language Processing
- [EMNLP'2021: SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://github.com/princeton-nlp/SimCSE) [论文](https://arxiv.org/pdf/2104.08821.pdf)
- [Contrastive Code Representation Learning: functionality-based JavaScript embeddings through self-supervised learning](https://github.com/parasj/contracode)
- [Multilingual Sentence & Image Embeddings with BERT](https://github.com/UKPLab/sentence-transformers)
- [DPTDR: Deep Prompt Tuning for Dense Passage Retrieval](https://arxiv.org/pdf/2208.11503.pdf)
- [RocketQA](https://github.com/PaddlePaddle/RocketQA)
- [RocketQAv2: A Joint Training Method for Dense Passage Retrieval
and Passage Re-ranking](https://arxiv.org/pdf/2110.07367.pdf)

## 标注

- [AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators](https://arxiv.org/abs/2303.16854)


## 安全

- [Is Power-Seeking AI an Existential Risk?](https://arxiv.org/pdf/2206.13353.pdf)

## 应用
- [CRSLab：可能是最适合你的对话推荐系统开源库
](https://picture.iczhiku.com/weixin/message1610089596644.html)

## 可解释

- [为什么说 GPT 是无损压缩](https://bigeagle.me/2023/03/llm-is-compression/)
- [Compression for AGI - Jack Rae | Stanford MLSys #76](https://www.youtube.com/watch?v=dO4TPJkeaaU)
## 新闻

- 2023-04-10 商汤发布商量大模型
- 2023-04-11 阿里发布通义大模型，阿里CEO张勇称阿里所有产品将用大模型改造。

