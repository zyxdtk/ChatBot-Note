# LLM


## 大模型

- [Deep Reinforcement Learning from Human Preferences](https://proceedings.neurips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf)
- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/pdf/2212.08073.pdf)
- [Scaling Laws for Reward Model Overoptimization](https://arxiv.org/pdf/2210.10760.pdf)
- [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/pdf/2303.17564.pdf)


- [TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION](https://arxiv.org/pdf/2108.12409.pdf)
- [让研究人员绞尽脑汁的Transformer位置编码](https://kexue.fm/archives/8130)

## Embedding

- [SGPT: GPT Sentence Embeddings for Semantic Search](https://arxiv.org/pdf/2202.08904v5.pdf)
- [Text and Code Embeddings by Contrastive Pre-Training](https://arxiv.org/pdf/2201.10005.pdf)   contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code
- [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/pdf/2004.04906.pdf)
- [sbert](www.sbert.net) 句向量模型
- [Title:Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)  [sentence-transformers](https://github.com/UKPLab/sentence-transformers)

## 标注

- [AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators](https://arxiv.org/abs/2303.16854)


## 安全

- [Is Power-Seeking AI an Existential Risk?](https://arxiv.org/pdf/2206.13353.pdf)

## 可解释

- [为什么说 GPT 是无损压缩](https://bigeagle.me/2023/03/llm-is-compression/)
## 新闻

- 2023-04-10 商汤发布商量大模型
- 2023-04-11 阿里发布通义大模型，阿里CEO张勇称阿里所有产品将用大模型改造。

